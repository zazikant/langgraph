{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Setup Environment and Dependencies\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Create .env directory if it doesn't exist\n",
        "os.makedirs('.env', exist_ok=True)\n",
        "\n",
        "# Write environment file\n",
        "env_content = \"\"\"GOOGLE_API_KEY=xxxxxx\n",
        "SERPER_API_KEY=xxxxxx\"\"\"\n",
        "\n",
        "with open('.env/.env', 'w') as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "print(\"‚úÖ Environment file created successfully!\")\n",
        "\n",
        "# Install required packages\n",
        "packages = [\n",
        "    \"langgraph\",\n",
        "    \"langchain\",\n",
        "    \"langchain-google-genai\",\n",
        "    \"beautifulsoup4\",\n",
        "    \"requests\",\n",
        "    \"python-dotenv\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    subprocess.run([\"pip\", \"install\", package], capture_output=True)\n",
        "    print(f\"‚úÖ Installed {package}\")\n",
        "\n",
        "print(\"üöÄ Setup complete! Run the next cells.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djULRrK6ZMP9",
        "outputId": "ee7180cd-1e9d-42f6-8cba-6d43f8cdc216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment file created successfully!\n",
            "‚úÖ Installed langgraph\n",
            "‚úÖ Installed langchain\n",
            "‚úÖ Installed langchain-google-genai\n",
            "‚úÖ Installed beautifulsoup4\n",
            "‚úÖ Installed requests\n",
            "‚úÖ Installed python-dotenv\n",
            "üöÄ Setup complete! Run the next cells.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgihv2q7SbO1",
        "outputId": "6ffa8d62-24e1-4ef7-c453-6020e7e652b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ COMPLETE FIXED SYSTEM READY!\n",
            "üéØ Edge function now correctly placed RIGHT AFTER classifier\n",
            "üìä Flow: Classifier ‚Üí Edge Router ‚Üí Research/Scraper ‚Üí Summary\n",
            "\n",
            "üöÄ Test with:\n",
            "execute_complete_fixed(\"research about gemengserv PMC and provide summary\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# COMPLETE FIXED SYSTEM - Copy and run this entire code\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.docstore.document import Document\n",
        "import json\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('.env/.env')\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# FIXED State Schema\n",
        "class ResearchState(TypedDict):\n",
        "    original_query: str\n",
        "    intent: Literal[\"research_only\", \"scrape_only\", \"research_and_scrape\"]\n",
        "    urls_to_scrape: list[str]\n",
        "    search_query: str\n",
        "    search_results: list[dict]\n",
        "    scraped_content: str\n",
        "    final_summary: str\n",
        "\n",
        "# FIXED APP-LEVEL ROUTER\n",
        "def app_level_router(query: str) -> str:\n",
        "    \"\"\"LLM-powered graph selection\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this user query and determine workflow:\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Rules:\n",
        "    - If query contains research + (summary OR summarize) ‚Üí \"research_and_summary\"\n",
        "    - If query only asks for research ‚Üí \"research_only\"\n",
        "    - If query asks to summarize provided text ‚Üí \"summary_only\"\n",
        "\n",
        "    Examples:\n",
        "    - \"research about X and provide summary\" ‚Üí research_and_summary\n",
        "    - \"find information about X\" ‚Üí research_only\n",
        "    - \"summarize this text: ...\" ‚Üí summary_only\n",
        "\n",
        "    Respond with exactly one option:\n",
        "    research_only\n",
        "    summary_only\n",
        "    research_and_summary\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    decision = response.content.strip().lower()\n",
        "\n",
        "    if \"research_and_summary\" in decision:\n",
        "        return \"research_and_summary\"\n",
        "    elif \"research_only\" in decision:\n",
        "        return \"research_only\"\n",
        "    elif \"summary_only\" in decision:\n",
        "        return \"summary_only\"\n",
        "    else:\n",
        "        return \"research_and_summary\"  # Default fallback\n",
        "\n",
        "# FIXED CLASSIFIER NODE\n",
        "def classifier_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Classify user intent - this determines the edge routing\"\"\"\n",
        "    query = state[\"original_query\"]\n",
        "\n",
        "    # Pre-check for research+summary pattern\n",
        "    needs_summary = any([\n",
        "        \"summary\" in query.lower(),\n",
        "        \"summarize\" in query.lower(),\n",
        "        \"provide summary\" in query.lower()\n",
        "    ]) and \"research\" in query.lower()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Classify this research query:\n",
        "    Query: \"{query}\"\n",
        "\n",
        "    Determine:\n",
        "    1. Intent (choose exactly one):\n",
        "       - \"research_only\" = just research, no summary needed\n",
        "       - \"research_and_scrape\" = research + scrape content for summary\n",
        "       - \"scrape_only\" = process provided URLs only\n",
        "\n",
        "    2. URLs: Extract any URLs or return \"empty\"\n",
        "    3. Search Query: Clean query for Google search\n",
        "\n",
        "    Format:\n",
        "    Intent: [choice]\n",
        "    URLs: [urls or empty]\n",
        "    Search Query: [clean terms]\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    result = response.content.strip()\n",
        "\n",
        "    print(f\"üîç Classifier Response:\\n{result}\")\n",
        "\n",
        "    # Parse response\n",
        "    try:\n",
        "        lines = result.split('\\n')\n",
        "        intent = None\n",
        "        urls_text = \"empty\"\n",
        "        search_query = query\n",
        "\n",
        "        for line in lines:\n",
        "            if 'Intent:' in line:\n",
        "                intent = line.split(':', 1)[1].strip()\n",
        "            elif 'URLs:' in line:\n",
        "                urls_text = line.split(':', 1)[1].strip()\n",
        "            elif 'Search Query:' in line:\n",
        "                search_query = line.split(':', 1)[1].strip()\n",
        "\n",
        "        # Override with pattern detection if needed\n",
        "        if needs_summary and intent != \"research_and_scrape\":\n",
        "            intent = \"research_and_scrape\"\n",
        "            print(f\"üîÑ Override: Detected summary request, setting intent to research_and_scrape\")\n",
        "\n",
        "        urls = []\n",
        "        if urls_text.lower() != \"empty\":\n",
        "            urls = [url.strip() for url in urls_text.split(',')]\n",
        "\n",
        "        # Clean search query\n",
        "        search_query = search_query.replace(\"research about\", \"\").replace(\"provide summary\", \"\").strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Parsing error: {e}\")\n",
        "        intent = \"research_and_scrape\" if needs_summary else \"research_only\"\n",
        "        urls = []\n",
        "        search_query = query\n",
        "\n",
        "    print(f\"‚úÖ Final Classification:\")\n",
        "    print(f\"   Intent: {intent}\")\n",
        "    print(f\"   Search Query: {search_query}\")\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"intent\": intent,\n",
        "        \"urls_to_scrape\": urls,\n",
        "        \"search_query\": search_query\n",
        "    }\n",
        "\n",
        "# CORRECT EDGE FUNCTION - Used right after classifier\n",
        "def classifier_edge_router(state: ResearchState) -> str:\n",
        "    \"\"\"Edge function used RIGHT AFTER classifier node\"\"\"\n",
        "    intent = state[\"intent\"]\n",
        "\n",
        "    print(f\"üß≠ Edge Router (after classifier):\")\n",
        "    print(f\"   Intent from classifier: {intent}\")\n",
        "\n",
        "    if intent == \"research_only\":\n",
        "        print(\"   ‚Üí Route: research (no scraping needed)\")\n",
        "        return \"research_only\"\n",
        "    elif intent == \"scrape_only\":\n",
        "        print(\"   ‚Üí Route: scraper (direct scraping)\")\n",
        "        return \"scraper\"\n",
        "    elif intent == \"research_and_scrape\":\n",
        "        print(\"   ‚Üí Route: research (then will scrape)\")\n",
        "        return \"research_then_scrape\"\n",
        "    else:\n",
        "        print(\"   ‚Üí Route: research (fallback)\")\n",
        "        return \"research_only\"\n",
        "\n",
        "# RESEARCH NODE (unchanged)\n",
        "def api_research_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Perform Google search using Serper API\"\"\"\n",
        "    search_query = state[\"search_query\"]\n",
        "\n",
        "    print(f\"üîé Searching for: {search_query}\")\n",
        "\n",
        "    try:\n",
        "        url = \"https://google.serper.dev/search\"\n",
        "        payload = json.dumps({\"q\": search_query})\n",
        "        headers = {\n",
        "            'X-API-KEY': os.getenv(\"SERPER_API_KEY\"),\n",
        "            'Content-Type': 'application/json'\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, data=payload)\n",
        "        results = response.json()\n",
        "        search_results = results.get(\"organic\", [])[:5]\n",
        "\n",
        "        print(f\"‚úÖ Found {len(search_results)} search results\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Search error: {e}\")\n",
        "        search_results = [{\"title\": f\"Mock result for {search_query}\", \"link\": \"https://example.com\", \"snippet\": f\"Information about {search_query}\"}]\n",
        "\n",
        "    return {**state, \"search_results\": search_results}\n",
        "\n",
        "# SCRAPER NODE (unchanged)\n",
        "def api_scraper_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Scrape web pages\"\"\"\n",
        "    urls_to_scrape = state.get(\"urls_to_scrape\", [])\n",
        "\n",
        "    if not urls_to_scrape and state.get(\"search_results\"):\n",
        "        urls_to_scrape = [result[\"link\"] for result in state[\"search_results\"][:3]]\n",
        "\n",
        "    print(f\"üï∑Ô∏è Scraping {len(urls_to_scrape)} URLs...\")\n",
        "\n",
        "    scraped_content = \"\"\n",
        "    for url in urls_to_scrape:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.decompose()\n",
        "\n",
        "            text = soup.get_text()\n",
        "            lines = (line.strip() for line in text.splitlines())\n",
        "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "            text = ' '.join(chunk for chunk in chunks if chunk)[:2000]\n",
        "\n",
        "            scraped_content += f\"\\n\\n--- Content from {url} ---\\n{text}\"\n",
        "            print(f\"‚úÖ Scraped: {url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            scraped_content += f\"\\n\\n--- Error scraping {url}: {str(e)} ---\"\n",
        "            print(f\"‚ùå Error scraping {url}\")\n",
        "\n",
        "    return {**state, \"scraped_content\": scraped_content}\n",
        "\n",
        "# SUMMARY NODE\n",
        "def summarization_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Summarize using research content\"\"\"\n",
        "\n",
        "    print(\"üìù Creating summary...\")\n",
        "\n",
        "    # Use scraped content if available, otherwise search results\n",
        "    if state.get(\"scraped_content\"):\n",
        "        content = state[\"scraped_content\"]\n",
        "        print(\"‚úÖ Using scraped content\")\n",
        "    elif state.get(\"search_results\"):\n",
        "        content = \"\\n\\n\".join([\n",
        "            f\"Title: {r.get('title', '')}\\nURL: {r.get('link', '')}\\nSnippet: {r.get('snippet', '')}\"\n",
        "            for r in state[\"search_results\"]\n",
        "        ])\n",
        "        print(\"‚úÖ Using search results\")\n",
        "    else:\n",
        "        content = f\"Limited information available about: {state['original_query']}\"\n",
        "        print(\"‚ö†Ô∏è Using fallback content\")\n",
        "\n",
        "    summary_prompt = f\"\"\"\n",
        "    Create a comprehensive summary about \"{state.get('search_query', state['original_query'])}\" based on this research:\n",
        "\n",
        "    RESEARCH DATA:\n",
        "    {content}\n",
        "\n",
        "    Provide:\n",
        "    1. Overview of the topic\n",
        "    2. Key findings from research\n",
        "    3. Important details\n",
        "    4. Conclusion\n",
        "\n",
        "    SUMMARY:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke([HumanMessage(content=summary_prompt)])\n",
        "        summary = response.content\n",
        "        print(\"‚úÖ Summary completed!\")\n",
        "    except Exception as e:\n",
        "        summary = f\"Summary error: {e}\\n\\nResearch data:\\n{content[:500]}...\"\n",
        "        print(f\"‚ùå Summary error: {e}\")\n",
        "\n",
        "    return {**state, \"final_summary\": summary}\n",
        "\n",
        "# CORRECTED GRAPH CONSTRUCTION\n",
        "def create_research_graph_corrected():\n",
        "    \"\"\"Research graph with CORRECT edge placement\"\"\"\n",
        "    workflow = StateGraph(ResearchState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"classifier\", classifier_node)\n",
        "    workflow.add_node(\"research\", api_research_node)\n",
        "    workflow.add_node(\"scraper\", api_scraper_node)\n",
        "\n",
        "    # Start with classifier\n",
        "    workflow.set_entry_point(\"classifier\")\n",
        "\n",
        "    # CRITICAL: Edge function RIGHT AFTER classifier\n",
        "    workflow.add_conditional_edges(\n",
        "        \"classifier\",  # FROM classifier\n",
        "        classifier_edge_router,  # Edge function\n",
        "        {\n",
        "            \"research_only\": \"research\",      # Just research, then end\n",
        "            \"research_then_scrape\": \"research\",  # Research first, then scrape\n",
        "            \"scraper\": \"scraper\"              # Direct to scraper\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # After research, check if we need to scrape\n",
        "    def post_research_router(state: ResearchState) -> str:\n",
        "        if state[\"intent\"] == \"research_and_scrape\":\n",
        "            return \"scraper\"\n",
        "        else:\n",
        "            return \"end\"\n",
        "\n",
        "    workflow.add_conditional_edges(\n",
        "        \"research\",\n",
        "        post_research_router,\n",
        "        {\"scraper\": \"scraper\", \"end\": END}\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(\"scraper\", END)\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "# MAIN EXECUTION\n",
        "def execute_complete_fixed(user_query: str):\n",
        "    \"\"\"Complete fixed execution\"\"\"\n",
        "\n",
        "    print(f\"üöÄ QUERY: {user_query}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # App-level routing\n",
        "    routing_decision = app_level_router(user_query)\n",
        "    print(f\"üß≠ App Router: {routing_decision}\")\n",
        "    print()\n",
        "\n",
        "    # Initialize state\n",
        "    initial_state = ResearchState(\n",
        "        original_query=user_query,\n",
        "        intent=\"research_only\",\n",
        "        urls_to_scrape=[],\n",
        "        search_query=\"\",\n",
        "        search_results=[],\n",
        "        scraped_content=\"\",\n",
        "        final_summary=\"\"\n",
        "    )\n",
        "\n",
        "    # Create graphs\n",
        "    research_graph = create_research_graph_corrected()\n",
        "\n",
        "    summary_workflow = StateGraph(ResearchState)\n",
        "    summary_workflow.add_node(\"summarize\", summarization_node)\n",
        "    summary_workflow.set_entry_point(\"summarize\")\n",
        "    summary_workflow.add_edge(\"summarize\", END)\n",
        "    summary_graph = summary_workflow.compile()\n",
        "\n",
        "    if routing_decision == \"research_and_summary\":\n",
        "        print(\"üìä EXECUTING: Research + Summary Pipeline\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Research phase\n",
        "        research_result = research_graph.invoke(initial_state)\n",
        "        print(f\"Research completed: {len(research_result.get('search_results', []))} results\")\n",
        "        print(f\"Content scraped: {'Yes' if research_result.get('scraped_content') else 'No'}\")\n",
        "\n",
        "        # Summary phase\n",
        "        final_result = summary_graph.invoke(research_result)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéØ FINAL SUMMARY:\")\n",
        "        print(\"=\" * 60)\n",
        "        print(final_result.get('final_summary'))\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    elif routing_decision == \"research_only\":\n",
        "        result = research_graph.invoke(initial_state)\n",
        "        print(\"üìä RESEARCH RESULTS:\")\n",
        "        for i, item in enumerate(result.get('search_results', []), 1):\n",
        "            print(f\"{i}. {item.get('title')}\")\n",
        "            print(f\"   {item.get('link')}\")\n",
        "            print()\n",
        "\n",
        "    elif routing_decision == \"summary_only\":\n",
        "        result = summary_graph.invoke(initial_state)\n",
        "        print(\"üìã SUMMARY:\")\n",
        "        print(result.get('final_summary'))\n",
        "\n",
        "print(\"‚úÖ COMPLETE FIXED SYSTEM READY!\")\n",
        "print(\"üéØ Edge function now correctly placed RIGHT AFTER classifier\")\n",
        "print(\"üìä Flow: Classifier ‚Üí Edge Router ‚Üí Research/Scraper ‚Üí Summary\")\n",
        "print(\"\\nüöÄ Test with:\")\n",
        "print('execute_complete_fixed(\"research about gemengserv PMC and provide summary\")')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execute_complete_fixed(\"go to gemengserv.com and extract only urls appearing in menu and provide summary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QRHszIEZjpf",
        "outputId": "7c7c6dd5-86c0-437a-a680-c3d5fd7f1a42",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ QUERY: go to gemengserv.com and extract only urls appearing in menu and provide summary\n",
            "============================================================\n",
            "üß≠ App Router: research_and_summary\n",
            "\n",
            "üìä EXECUTING: Research + Summary Pipeline\n",
            "----------------------------------------\n",
            "üîç Classifier Response:\n",
            "Intent: research_and_scrape\n",
            "URLs: gemengserv.com\n",
            "Search Query: gemengserv.com website menu links\n",
            "‚úÖ Final Classification:\n",
            "   Intent: research_and_scrape\n",
            "   Search Query: gemengserv.com website menu links\n",
            "üß≠ Edge Router (after classifier):\n",
            "   Intent from classifier: research_and_scrape\n",
            "   ‚Üí Route: research (then will scrape)\n",
            "üîé Searching for: gemengserv.com website menu links\n",
            "‚úÖ Found 5 search results\n",
            "üï∑Ô∏è Scraping 1 URLs...\n",
            "‚ùå Error scraping gemengserv.com\n",
            "Research completed: 5 results\n",
            "Content scraped: Yes\n",
            "üìù Creating summary...\n",
            "‚úÖ Using scraped content\n",
            "‚úÖ Summary completed!\n",
            "\n",
            "============================================================\n",
            "üéØ FINAL SUMMARY:\n",
            "============================================================\n",
            "## Summary of gemengserv.com Website Menu Links\n",
            "\n",
            "**1. Overview of the Topic:**\n",
            "\n",
            "This summary aims to describe the menu links found on the website gemengserv.com.  The initial research attempt failed due to an invalid URL provided (missing \"https://\").  Therefore, this summary cannot provide specific details about the website's menu structure and content.  It will instead focus on a hypothetical analysis of what one might expect to find on a website with a name suggesting \"Gemeng Serv\" (likely implying services related to gemology or a similar field).\n",
            "\n",
            "**2. Key Findings from Research (Hypothetical):**\n",
            "\n",
            "Since direct research was impossible due to the URL error, the following are hypothetical key findings based on the website name:\n",
            "\n",
            "* **Service-Oriented Structure:**  The menu would likely be organized around the services offered.  This could include categories such as gemstone appraisal, jewelry repair, custom design, gemstone sourcing, educational resources, or a contact section.\n",
            "* **Informational Pages:**  The menu might also include pages providing information about the company, its history, team, or certifications.  A blog or news section could also be present.\n",
            "* **Client Portal (Possible):**  Depending on the nature of the services, a client portal for accessing orders, appointments, or invoices might be accessible through a menu link.\n",
            "* **E-commerce Functionality (Possible):** If the company sells gemstones or jewelry, a \"Shop\" or \"Products\" link would be expected in the menu.\n",
            "\n",
            "\n",
            "**3. Important Details (Hypothetical):**\n",
            "\n",
            "The specific wording of the menu links would depend on the company's branding and target audience.  However, we can hypothesize some potential menu items:\n",
            "\n",
            "* **Services:** Gemstone Identification, Jewelry Repair & Restoration, Custom Jewelry Design, Gemstone Procurement, Appraisal Services\n",
            "* **About Us:** Our Story, Team, Certifications, Contact Us\n",
            "* **Resources:** Blog, FAQs, Glossary of Terms\n",
            "* **Shop (if applicable):** Gemstones, Jewelry, Accessories\n",
            "\n",
            "\n",
            "**4. Conclusion:**\n",
            "\n",
            "Without access to the actual gemengserv.com website due to the URL error, this summary provides a hypothetical analysis of its potential menu structure based on the website name.  A properly formatted URL (e.g., `https://gemengserv.com`) is needed to conduct accurate research and provide a factual summary of the website's menu links.  The hypothetical analysis suggests a menu structure focused on services related to gemology, potentially including e-commerce functionality and informational pages.  Further research with a correct URL is necessary to confirm these assumptions.\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}